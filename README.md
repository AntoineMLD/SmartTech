SmartTech ‚Äì Pipeline IoT Temps R√©el avec Spark, Kafka et Delta Lake

Contexte du Projet
SmartTech simule un syst√®me IoT pour b√¢timents intelligents, avec :

Capteurs (temp√©rature, humidit√©, √©nergie, CO2).
Traitement temps r√©el pour d√©tecter des anomalies et alimenter des tableaux de bord.
Historisation des donn√©es pour analyses ult√©rieures.
Ce d√©p√¥t impl√©mente deux pipelines pour r√©pondre √† un brief acad√©mique en deux parties :


# SmartTech ‚Äì Pipeline IoT Temps R√©el avec Spark, Kafka et Delta Lake

Biblioth√®ques Python :


pip install pyspark delta-spark kafka-python
Ajoute ces lignes √† ton ~/.bashrc ou ~/.zshrc :

export JAVA_HOME="/usr/lib/jvm/java-17-openjdk-amd64/"
export PATH="$JAVA_HOME/bin:$PATH"


source ~/.bashrc  # ou source ~/.zshrc


Utilisation
# SmartTech ‚Äì Pipeline IoT Temps R√©el avec Spark, Kafka et Delta Lake


## üöÄ Contexte du Projet

SmartTech simule un syst√®me IoT pour b√¢timents intelligents :

- **Capteurs** (temp√©rature, humidit√©, √©nergie, CO2)
- **Traitement temps r√©el** pour d√©tecter des anomalies et alimenter des tableaux de bord
- **Historisation** des donn√©es pour analyses ult√©rieures

Ce d√©p√¥t impl√©mente deux pipelines pour r√©pondre √† un brief acad√©mique :
1. **Veille** sur le streaming structur√© avec Spark (concepts cl√©s, architecture M√©daillon)
2. **Mise en pratique** avec des pipelines Spark + Kafka

---

## üìÅ Structure du D√©p√¥t

| Fichier/Script                | R√¥le                                                        |
|------------------------------|-------------------------------------------------------------|
| `pipeline_streaming_simple.py`| Pipeline Spark local (fichiers JSON ‚Üí Delta Bronze)         |
| `kafka_producer.py`           | Simulateur de capteurs (envoie des donn√©es dans Kafka)      |
| `spark_kafka_consumer.py`     | Pipeline Spark Streaming (Kafka ‚Üí Delta Silver)             |
| `read_delta.py`               | Lecture des tables Delta (Bronze/Silver)                    |
| `docker-compose.yml`          | D√©ploiement local de Kafka/Zookeeper                        |
| `sensor_data/`                | Dossier pour les fichiers JSON (mode local)                 |
| `sensor_data_bronze/`         | Table Delta Bronze (donn√©es brutes)                         |
| `sensor_data_silver/`         | Table Delta Silver (donn√©es nettoy√©es)                      |

---

## üõ†Ô∏è Pr√©requis

- Python 3.8+
- Java 11/17 (OpenJDK recommand√©)
- Docker (pour Kafka/Zookeeper)
- Biblioth√®ques Python :

```bash
pip install pyspark delta-spark kafka-python
```

### Configuration Java

Ajoutez √† votre `~/.bashrc` ou `~/.zshrc` :

```bash
export JAVA_HOME="/usr/lib/jvm/java-17-openjdk-amd64/"
export PATH="$JAVA_HOME/bin:$PATH"
```

Puis rechargez votre shell :

```bash
source ~/.bashrc  # ou source ~/.zshrc
```

---

## ‚ñ∂Ô∏è Utilisation

### Pipeline Local (Fichiers JSON ‚Üí Delta Bronze)

1. Placez des fichiers JSON dans `sensor_data/` (exemple ci-dessous)
2. Lancez le pipeline :
      ```bash
      python3 pipeline_streaming_simple.py
      ```
      Les donn√©es sont √©crites dans `sensor_data_bronze/` (format Delta)

#### Exemple de fichier JSON (`sensor_data/test.json`) :

```json
{
  "timestamp": "2025-12-15T10:00:00Z",
  "device_id": "sensor_001",
  "building": "A",
  "floor": 2,
  "type": "temperature",
  "value": 22.5,
  "unit": "C"
}
```

### Pipeline Kafka (Temps R√©el)

1. **D√©marrer Kafka/Zookeeper**
      ```bash
      docker-compose up -d
      ```
2. **Lancer le producteur Kafka (simulateur de capteurs)**
      ```bash
      python3 kafka_producer.py
      ```
      G√©n√®re des donn√©es al√©atoires (1 message/seconde) dans le topic `sensor_data`.
3. **Lancer le consommateur Spark**
      ```bash
      python3 spark_kafka_consumer.py
      ```
      Consomme les messages Kafka, les nettoie, et les √©crit dans `sensor_data_silver/` (Delta Silver).
4. **Lire les donn√©es Delta**
      ```bash
      python3 read_delta.py
      ```
      Affiche les donn√©es stock√©es dans Delta.

---

## üèõÔ∏è Architecture M√©daillon

| Niveau   | Description                                         | Dossier/Table           |
|----------|-----------------------------------------------------|-------------------------|
| Bronze   | Donn√©es brutes (peu ou pas transform√©es)            | `sensor_data_bronze/`   |
| Silver   | Donn√©es nettoy√©es (timestamps corrig√©s, sch√©mas valid√©s) | `sensor_data_silver/`   |
| Gold     | (Optionnel) Agr√©gations (moyennes, anomalies)       | √Ä impl√©menter           |

---

## üß© Concepts Cl√©s

### Pourquoi Kafka ?

| Avantage         | Explication                                                        |
|------------------|--------------------------------------------------------------------|
| D√©couplage       | Producteurs et consommateurs sont ind√©pendants                     |
| Scalabilit√©      | G√®re des millions de messages/seconde                              |
| Persistance      | Les messages sont stock√©s durablement (contrairement √† un dossier) |
| Ordre garanti    | Dans une partition, l'ordre des messages est pr√©serv√©              |
| Reprise apr√®s panne | Gr√¢ce aux offsets (position dans la partition)                  |

### Termes Kafka

| Terme           | R√¥le                                                               |
|-----------------|--------------------------------------------------------------------|
| Topic           | Canal de communication (ex: `sensor_data`)                         |
| Partition       | Sous-division d'un topic pour parall√©liser la lecture/√©criture     |
| Offset          | Position d'un message dans une partition (ex: offset=42)           |
| Consumer Group  | Groupe de consommateurs qui se partagent les partitions            |

---

## üìù Notes Techniques

- **Arr√™t des flux** : Les scripts tournent en continu. Utilisez `Ctrl+C` pour les arr√™ter.
- **Checkpointing** : Spark stocke les offsets Kafka dans `/tmp/kafka_checkpoint` pour la reprise apr√®s panne.
- **Partitionnement Delta** : Les tables sont partitionn√©es par `building` et `type` pour des requ√™tes optimis√©es.

---
